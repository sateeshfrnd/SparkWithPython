{"cells":[{"cell_type":"markdown","source":["## Partitioning a DataFrame\nPySpark DataFrame can be split into the specifed number of partitions using **repartition(~)** method. This method allows to partition by column values.\n\n### Parameters\n1. numPartitions | int\n    - Number of patitions to break down the DataFrame.\n\n2. cols or str or Column\n    - Columns by which to partition the DataFrame.\n\n### Return Value\n  - A new PySpark DataFrame.\n\n[API Reference](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.repartition.html)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eb06c0a5-d367-4fa5-a201-a7671e6942a6"}}},{"cell_type":"markdown","source":["### Example\n\n## Partitioning DataFrame"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1957588f-1e66-44bf-a1b3-416fa5788a45"}}},{"cell_type":"code","source":["df = spark.createDataFrame(\n  [('Shiva', 'Appu'), ('Teja', 'Sweety'), ('Bhavishya', 'Bhavi'), ('Shiva', 'Chinu')],\n  ['name','nick_name']\n)\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"96430ebd-8226-4246-a10a-fb99864f0245"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---------+---------+\n|name     |nick_name|\n+---------+---------+\n|Teja     |Sweety   |\n|Bhavishya|Bhavi    |\n|Shiva    |Appu     |\n|Shiva    |Chinu    |\n+---------+---------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---------+---------+\n|name     |nick_name|\n+---------+---------+\n|Teja     |Sweety   |\n|Bhavishya|Bhavi    |\n|Shiva    |Appu     |\n|Shiva    |Chinu    |\n+---------+---------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# Get the number of partitions\ndf.rdd.getNumPartitions()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"453a75a2-7366-404c-90c0-7e51507c3bba"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[24]: 8","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[24]: 8"]}}],"execution_count":0},{"cell_type":"markdown","source":["DataFrame is split into 8 partitions by default.Since, the number of partitions depends on the parallelism level of your Spark configurations."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4b013570-b63c-48cb-9c40-c2c02c13c59e"}}},{"cell_type":"code","source":["df.rdd.glom().collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a271af97-e88d-4f28-a511-095af5c53e6c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[25]: [[],\n [Row(name='Teja', nick_name='Sweety')],\n [],\n [Row(name='Bhavishya', nick_name='Bhavi')],\n [],\n [Row(name='Shiva', nick_name='Appu')],\n [],\n [Row(name='Shiva', nick_name='Chinu')]]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[25]: [[],\n [Row(name='Teja', nick_name='Sweety')],\n [],\n [Row(name='Bhavishya', nick_name='Bhavi')],\n [],\n [Row(name='Shiva', nick_name='Appu')],\n [],\n [Row(name='Shiva', nick_name='Chinu')]]"]}}],"execution_count":0},{"cell_type":"markdown","source":["Here, we can see that we have indeed 8 partitions, but only 4 of the partitions have a Row in them.\n\nNow, let's repartition our DataFrame such that the Rows are divided into only 2 partitions using 'repartition' method:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1de065c3-26a0-4219-850e-623755846948"}}},{"cell_type":"code","source":["df1 = df.repartition(2)\ndf1.rdd.getNumPartitions()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"119dd5be-d43f-4302-a473-5604e22314ce"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[26]: 2","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[26]: 2"]}}],"execution_count":0},{"cell_type":"markdown","source":["Now the distribution of the rows are in repartitioned DataFrame(d1)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b52747ff-c667-45de-a2d4-27d382cc7b6c"}}},{"cell_type":"code","source":["df1.rdd.glom().collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ffa45ece-917e-4c6f-809a-0279a19ecfd3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[27]: [[Row(name='Teja', nick_name='Sweety'),\n  Row(name='Bhavishya', nick_name='Bhavi'),\n  Row(name='Shiva', nick_name='Appu'),\n  Row(name='Shiva', nick_name='Chinu')],\n []]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[27]: [[Row(name='Teja', nick_name='Sweety'),\n  Row(name='Bhavishya', nick_name='Bhavi'),\n  Row(name='Shiva', nick_name='Appu'),\n  Row(name='Shiva', nick_name='Chinu')],\n []]"]}}],"execution_count":0},{"cell_type":"markdown","source":["> **Note:** *No guarantee that the rows will be evenly distributed in the partitions*"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ba4c2734-f185-4435-9937-04317e6793cc"}}},{"cell_type":"markdown","source":["## Partitioning DataFrame by columns"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9b37626f-985c-476a-9ae0-9e974845e621"}}},{"cell_type":"code","source":["df.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9f9bb43d-df43-41d0-9d4f-3593c11231de"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---------+---------+\n|name     |nick_name|\n+---------+---------+\n|Teja     |Sweety   |\n|Bhavishya|Bhavi    |\n|Shiva    |Appu     |\n|Shiva    |Chinu    |\n+---------+---------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---------+---------+\n|name     |nick_name|\n+---------+---------+\n|Teja     |Sweety   |\n|Bhavishya|Bhavi    |\n|Shiva    |Appu     |\n|Shiva    |Chinu    |\n+---------+---------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# repartition DataFrame by the column 'name' \n\ndf3 = df.repartition(\"name\")\nprint(f'Number of Partitions = {df3.rdd.getNumPartitions()}')\ndf3.rdd.glom().collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a00c3c1a-785d-4ec2-8561-8769aef1e829"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Number of Partitions = 1\nOut[31]: [[Row(name='Teja', nick_name='Sweety'),\n  Row(name='Bhavishya', nick_name='Bhavi'),\n  Row(name='Shiva', nick_name='Appu'),\n  Row(name='Shiva', nick_name='Chinu')]]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Number of Partitions = 1\nOut[31]: [[Row(name='Teja', nick_name='Sweety'),\n  Row(name='Bhavishya', nick_name='Bhavi'),\n  Row(name='Shiva', nick_name='Appu'),\n  Row(name='Shiva', nick_name='Chinu')]]"]}}],"execution_count":0},{"cell_type":"code","source":["# repartition DataFrame by the column 'name' into 2 partitions\ndf4 = df.repartition(2, \"name\")\nprint(f'Number of Partitions = {df4.rdd.getNumPartitions()}')\ndf4.rdd.glom().collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4a91a914-fa3b-46e0-8e56-5e706ca9aa18"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Number of Partitions = 2\nOut[32]: [[],\n [Row(name='Teja', nick_name='Sweety'),\n  Row(name='Bhavishya', nick_name='Bhavi'),\n  Row(name='Shiva', nick_name='Appu'),\n  Row(name='Shiva', nick_name='Chinu')]]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Number of Partitions = 2\nOut[32]: [[],\n [Row(name='Teja', nick_name='Sweety'),\n  Row(name='Bhavishya', nick_name='Bhavi'),\n  Row(name='Shiva', nick_name='Appu'),\n  Row(name='Shiva', nick_name='Chinu')]]"]}}],"execution_count":0},{"cell_type":"code","source":["# repartition DataFrame by the multiple columns\ndf5 = df.repartition(2, \"name\", \"nick_name\")\nprint(f'Number of Partitions = {df5.rdd.getNumPartitions()}')\ndf5.rdd.glom().collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d5412e51-9d78-48d7-b128-230968c3d437"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Number of Partitions = 2\nOut[34]: [[Row(name='Teja', nick_name='Sweety'), Row(name='Shiva', nick_name='Appu')],\n [Row(name='Bhavishya', nick_name='Bhavi'),\n  Row(name='Shiva', nick_name='Chinu')]]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Number of Partitions = 2\nOut[34]: [[Row(name='Teja', nick_name='Sweety'), Row(name='Shiva', nick_name='Appu')],\n [Row(name='Bhavishya', nick_name='Bhavi'),\n  Row(name='Shiva', nick_name='Chinu')]]"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b169f151-503e-4559-adbe-d173b74fd7b3"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"PySpark DataFrame - Repartition","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3717608978273847}},"nbformat":4,"nbformat_minor":0}
