{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "787f9a60-0535-4ef6-a29c-3843fbec63be",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Problem :\n",
    "Create a new column with key-value similar to Python Dictonary based on exisiting columns from the DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2fd6533b-998e-4d75-9268-e5b6668405fd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Solution:\n",
    "In Spark 2.0 or later versions, PySpark built in SQL function **'create_map'** will be used to convert selected columns of the DataFrame to **MapType**. Function create_map() takes a list of columns that are grouped as key-value pairs.\n",
    "\n",
    "**API Reference:**\n",
    "https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.functions.create_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "04324608-5c6e-4dc2-a285-d50f5c724cb0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a73d56d7-3c13-49b0-b61b-14be2e97f750",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-O9UJUJO.mshome.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>DataFrame Functions</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1534c3d7d30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DataFrame Functions\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4cffa046-2231-4665-8499-521a35470110",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Create a DataFrame with Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1a99f3e4-8bb5-4a96-a4c5-fd579b6bea6f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- course: string (nullable = true)\n",
      " |-- fee: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "     StructField('year', StringType(), True),\n",
    "     StructField('course', StringType(), True),\n",
    "     StructField('fee', IntegerType(), True),    \n",
    "     ])\n",
    "\n",
    "data = [\n",
    "       (2022,'Spark',15000),(2022,'BigData',10000),(2022,'Scala',10000),(2022,'Python',10000),(2022,'Java',10000), (2022,'DevOps',15000), (2022,'AWS',20000),(2022,'ML',35000),\n",
    "       (2021,'Spark',15000),(2021,'BigData',10000),(2021,'Scala',10000),(2021,'Python',10000),(2021,'Java',10000), (2021,'DevOps',15000), (2021,'AWS',20000),(2021,'ML',35000),\n",
    "       (2020,'Spark',15000),(2020,'BigData',10000),(2020,'Scala',10000),(2020,'Python',10000),(2020,'Java',10000), (2020,'DevOps',15000), (2020,'AWS',20000),(2020,'ML',30000),\n",
    "       ]\n",
    "\n",
    "courses_df = spark.createDataFrame(data,schema)\n",
    "courses_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----+\n",
      "|year| course|  fee|\n",
      "+----+-------+-----+\n",
      "|2022|  Spark|15000|\n",
      "|2022|BigData|10000|\n",
      "|2022|  Scala|10000|\n",
      "|2022| Python|10000|\n",
      "|2022|   Java|10000|\n",
      "|2022| DevOps|15000|\n",
      "|2022|    AWS|20000|\n",
      "|2022|     ML|35000|\n",
      "|2021|  Spark|15000|\n",
      "|2021|BigData|10000|\n",
      "|2021|  Scala|10000|\n",
      "|2021| Python|10000|\n",
      "|2021|   Java|10000|\n",
      "|2021| DevOps|15000|\n",
      "|2021|    AWS|20000|\n",
      "|2021|     ML|35000|\n",
      "|2020|  Spark|15000|\n",
      "|2020|BigData|10000|\n",
      "|2020|  Scala|10000|\n",
      "|2020| Python|10000|\n",
      "+----+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "courses_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "62b0de9e-2271-41ed-98e0-2f3365e1f310",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Convert DataFrame columns to MapType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b2f918c7-7cb8-44c1-9ce5-17f0cff591c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- course_details: map (nullable = false)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,lit,create_map\n",
    "course_details = (\n",
    "  courses_df.withColumn(\"course_details\",create_map(\n",
    "        lit(\"course\"),col(\"course\"),\n",
    "        lit(\"fee\"),col(\"fee\")\n",
    "        )).drop(\"course\",\"fee\")\n",
    ")\n",
    "course_details.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "62c71be7-e128-4b4e-86a3-4314be3a7226",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------------------------+\n",
      "|year|course_details                   |\n",
      "+----+---------------------------------+\n",
      "|2022|{course -> Spark, fee -> 15000}  |\n",
      "|2022|{course -> BigData, fee -> 10000}|\n",
      "|2022|{course -> Scala, fee -> 10000}  |\n",
      "|2022|{course -> Python, fee -> 10000} |\n",
      "|2022|{course -> Java, fee -> 10000}   |\n",
      "|2022|{course -> DevOps, fee -> 15000} |\n",
      "|2022|{course -> AWS, fee -> 20000}    |\n",
      "|2022|{course -> ML, fee -> 35000}     |\n",
      "|2021|{course -> Spark, fee -> 15000}  |\n",
      "|2021|{course -> BigData, fee -> 10000}|\n",
      "|2021|{course -> Scala, fee -> 10000}  |\n",
      "|2021|{course -> Python, fee -> 10000} |\n",
      "|2021|{course -> Java, fee -> 10000}   |\n",
      "|2021|{course -> DevOps, fee -> 15000} |\n",
      "|2021|{course -> AWS, fee -> 20000}    |\n",
      "|2021|{course -> ML, fee -> 35000}     |\n",
      "|2020|{course -> Spark, fee -> 15000}  |\n",
      "|2020|{course -> BigData, fee -> 10000}|\n",
      "|2020|{course -> Scala, fee -> 10000}  |\n",
      "|2020|{course -> Python, fee -> 10000} |\n",
      "+----+---------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "course_details.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Create MapType Column from existing columns",
   "notebookOrigID": 1625101934297447,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
