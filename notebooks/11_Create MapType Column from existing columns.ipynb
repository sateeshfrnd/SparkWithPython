{"cells":[{"cell_type":"markdown","source":["## Problem :\nCreate a new column with key-value similar to Python Dictonary based on exisiting columns from the DataFrame."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"787f9a60-0535-4ef6-a29c-3843fbec63be"}}},{"cell_type":"markdown","source":["## Solution:\nIn Spark 2.0 or later versions, PySpark built in SQL function **'create_map'** will be used to convert selected columns of the DataFrame to **MapType**. Function create_map() takes a list of columns that are grouped as key-value pairs.\n\n**API Reference:**\nhttps://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.functions.create_map"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2fd6533b-998e-4d75-9268-e5b6668405fd"}}},{"cell_type":"markdown","source":["## Implementation"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"04324608-5c6e-4dc2-a285-d50f5c724cb0"}}},{"cell_type":"code","source":["spark"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a73d56d7-3c13-49b0-b61b-14be2e97f750"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=3073909438274297#setting/sparkui/0331-023229-5eesesee/driver-6005892204633439464\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.1.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=3073909438274297#setting/sparkui/0331-023229-5eesesee/driver-6005892204633439464\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.1.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Create a DataFrame with Sample Data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4cffa046-2231-4665-8499-521a35470110"}}},{"cell_type":"code","source":["from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n\nschema = StructType([\n     StructField('year', StringType(), True),\n     StructField('course', StringType(), True),\n     StructField('fee', IntegerType(), True),    \n     ])\n\ndata = [\n       (2022,'Spark',15000),(2022,'BigData',10000),(2022,'Scala',10000),(2022,'Python',10000),(2022,'Java',10000), (2022,'DevOps',15000), (2022,'AWS',20000),(2022,'ML',35000),\n       (2021,'Spark',15000),(2021,'BigData',10000),(2021,'Scala',10000),(2021,'Python',10000),(2021,'Java',10000), (2021,'DevOps',15000), (2021,'AWS',20000),(2021,'ML',35000),\n       (2020,'Spark',15000),(2020,'BigData',10000),(2020,'Scala',10000),(2020,'Python',10000),(2020,'Java',10000), (2020,'DevOps',15000), (2020,'AWS',20000),(2020,'ML',30000),\n       ]\n\ncourses_df = spark.createDataFrame(data,schema)\ncourses_df.printSchema()\ncourses_df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1a99f3e4-8bb5-4a96-a4c5-fd579b6bea6f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- year: string (nullable = true)\n |-- course: string (nullable = true)\n |-- fee: integer (nullable = true)\n\n+----+-------+-----+\n|year| course|  fee|\n+----+-------+-----+\n|2022|  Spark|15000|\n|2022|BigData|10000|\n|2022|  Scala|10000|\n|2022| Python|10000|\n|2022|   Java|10000|\n|2022| DevOps|15000|\n|2022|    AWS|20000|\n|2022|     ML|35000|\n|2021|  Spark|15000|\n|2021|BigData|10000|\n|2021|  Scala|10000|\n|2021| Python|10000|\n|2021|   Java|10000|\n|2021| DevOps|15000|\n|2021|    AWS|20000|\n|2021|     ML|35000|\n|2020|  Spark|15000|\n|2020|BigData|10000|\n|2020|  Scala|10000|\n|2020| Python|10000|\n+----+-------+-----+\nonly showing top 20 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- year: string (nullable = true)\n |-- course: string (nullable = true)\n |-- fee: integer (nullable = true)\n\n+----+-------+-----+\n|year| course|  fee|\n+----+-------+-----+\n|2022|  Spark|15000|\n|2022|BigData|10000|\n|2022|  Scala|10000|\n|2022| Python|10000|\n|2022|   Java|10000|\n|2022| DevOps|15000|\n|2022|    AWS|20000|\n|2022|     ML|35000|\n|2021|  Spark|15000|\n|2021|BigData|10000|\n|2021|  Scala|10000|\n|2021| Python|10000|\n|2021|   Java|10000|\n|2021| DevOps|15000|\n|2021|    AWS|20000|\n|2021|     ML|35000|\n|2020|  Spark|15000|\n|2020|BigData|10000|\n|2020|  Scala|10000|\n|2020| Python|10000|\n+----+-------+-----+\nonly showing top 20 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Convert DataFrame columns to MapType"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"62b0de9e-2271-41ed-98e0-2f3365e1f310"}}},{"cell_type":"code","source":["from pyspark.sql.functions import col,lit,create_map\ncourse_details = (\n  courses_df.withColumn(\"course_details\",create_map(\n        lit(\"course\"),col(\"course\"),\n        lit(\"fee\"),col(\"fee\")\n        )).drop(\"course\",\"fee\")\n)\ncourse_details.printSchema()\ncourse_details.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b2f918c7-7cb8-44c1-9ce5-17f0cff591c1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- year: string (nullable = true)\n |-- course_details: map (nullable = false)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n+----+---------------------------------+\n|year|course_details                   |\n+----+---------------------------------+\n|2022|{course -> Spark, fee -> 15000}  |\n|2022|{course -> BigData, fee -> 10000}|\n|2022|{course -> Scala, fee -> 10000}  |\n|2022|{course -> Python, fee -> 10000} |\n|2022|{course -> Java, fee -> 10000}   |\n|2022|{course -> DevOps, fee -> 15000} |\n|2022|{course -> AWS, fee -> 20000}    |\n|2022|{course -> ML, fee -> 35000}     |\n|2021|{course -> Spark, fee -> 15000}  |\n|2021|{course -> BigData, fee -> 10000}|\n|2021|{course -> Scala, fee -> 10000}  |\n|2021|{course -> Python, fee -> 10000} |\n|2021|{course -> Java, fee -> 10000}   |\n|2021|{course -> DevOps, fee -> 15000} |\n|2021|{course -> AWS, fee -> 20000}    |\n|2021|{course -> ML, fee -> 35000}     |\n|2020|{course -> Spark, fee -> 15000}  |\n|2020|{course -> BigData, fee -> 10000}|\n|2020|{course -> Scala, fee -> 10000}  |\n|2020|{course -> Python, fee -> 10000} |\n+----+---------------------------------+\nonly showing top 20 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- year: string (nullable = true)\n |-- course_details: map (nullable = false)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n+----+---------------------------------+\n|year|course_details                   |\n+----+---------------------------------+\n|2022|{course -> Spark, fee -> 15000}  |\n|2022|{course -> BigData, fee -> 10000}|\n|2022|{course -> Scala, fee -> 10000}  |\n|2022|{course -> Python, fee -> 10000} |\n|2022|{course -> Java, fee -> 10000}   |\n|2022|{course -> DevOps, fee -> 15000} |\n|2022|{course -> AWS, fee -> 20000}    |\n|2022|{course -> ML, fee -> 35000}     |\n|2021|{course -> Spark, fee -> 15000}  |\n|2021|{course -> BigData, fee -> 10000}|\n|2021|{course -> Scala, fee -> 10000}  |\n|2021|{course -> Python, fee -> 10000} |\n|2021|{course -> Java, fee -> 10000}   |\n|2021|{course -> DevOps, fee -> 15000} |\n|2021|{course -> AWS, fee -> 20000}    |\n|2021|{course -> ML, fee -> 35000}     |\n|2020|{course -> Spark, fee -> 15000}  |\n|2020|{course -> BigData, fee -> 10000}|\n|2020|{course -> Scala, fee -> 10000}  |\n|2020|{course -> Python, fee -> 10000} |\n+----+---------------------------------+\nonly showing top 20 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"62c71be7-e128-4b4e-86a3-4314be3a7226"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Create MapType Column from existing columns","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1625101934297447}},"nbformat":4,"nbformat_minor":0}
